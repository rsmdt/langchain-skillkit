# langchain-skillkit — API Redesign

**Date:** 2026-02-27
**Status:** Design complete, ready for specification

## Problem

The current `langchain-agent-skills` package has a `create_agent()` API that returns a plain callable node function. Users must manually wire `ToolNode`, conditional edges, and the ReAct loop. There is no mechanism for:

- Injecting external dependencies (DB, API clients, config)
- Accessing LangGraph Runtime (store, stream_writer, context)
- Controlling how the LLM is called
- Creating self-contained subgraphs that compose as nodes

## Design

### Package rename

`langchain-agent-skills` → `langchain-skillkit`

### Public API

| Export | Type | Purpose |
|---|---|---|
| `SkillKit` | `BaseToolkit` subclass | Standalone toolkit. Takes skill directories, returns `Skill` + `SkillRead` tools via `get_tools()`. For manual LangGraph wiring. |
| `node` | Metaclass | Convenience ReAct subgraph. Declares LLM, tools, skills. Produces a `CompiledStateGraph`. |
| `AgentState` | `TypedDict` | Minimal state with `messages` + `sender` |

### SkillKit (manual path)

For users who want to wire their own LangGraph graph:

```python
from langchain_skillkit import SkillKit

# Single directory
skill_kit = SkillKit("skills/")

# Multiple directories
skill_kit = SkillKit(["skills/", "shared_skills/"])

# Get tools — standard LangChain BaseToolkit pattern
skill_tools = skill_kit.get_tools()  # → [Skill, SkillRead]

# Use in any LangGraph setup
all_tools = [web_search, calculate] + skill_tools
bound_llm = llm.bind_tools(all_tools)
tool_node = ToolNode(all_tools)
```

`SkillKit` accepts `str | list[str]` for skill directory paths.

### The `node` metaclass (convenience path)

A class declaration that produces a `CompiledStateGraph` — usable as both a standalone graph and a node in a larger graph.

```python
from langchain_skillkit import node
from langchain_core.messages import SystemMessage
from langchain_openai import ChatOpenAI

PROMPT = "You are a research assistant helping with market analysis."

class researcher(node):
    llm = ChatOpenAI(model="gpt-4o")
    tools = [web_search, calculate]
    skills = "skills/"

    async def handler(state, *, llm, tools, runtime: Runtime[AppContext]):
        db = runtime.context.db
        user = await db.get_user(runtime.context.user_id)
        messages = [SystemMessage(PROMPT)] + state["messages"]
        response = await llm.ainvoke(messages)
        return {"messages": [response], "sender": "researcher"}
```

### Handler signature

Follows LangGraph convention: `state` positional, everything else keyword-only after `*`.

```python
async def handler(state, *, llm, tools, runtime: Runtime[AppContext])
```

The metaclass inspects the handler's signature and injects only what's requested, by name:

```python
# All valid — declare only what you need, any order after *:
async def handler(state, *, llm, tools, runtime): ...
async def handler(state, *, llm, runtime): ...
async def handler(state, *, runtime, llm): ...
async def handler(state, *, llm): ...
async def handler(state, *, runtime): ...
async def handler(state): ...
```

Available injectable parameters:

| Parameter | Type | Description |
|---|---|---|
| `state` | `dict` | Graph state (messages, sender). Always positional, always first. |
| `llm` | `Runnable` | LLM pre-bound with `Skill` + `SkillRead` + user tools |
| `tools` | `list[BaseTool]` | Full tool list that's bound to the LLM (for inspection) |
| `runtime` | `Runtime[ContextT]` | Standard LangGraph Runtime (context, store, stream_writer) |

The metaclass generates a real LangGraph node internally:

```python
# Generated by metaclass — what LangGraph sees:
async def _langgraph_node(state, *, runtime: Runtime[ContextT]):
    bound_llm = llm.bind_tools(all_tools)
    # Inspect handler signature, inject only requested params
    return await handler(state, llm=bound_llm, tools=all_tools, runtime=runtime)
```

### What the metaclass produces

A `CompiledStateGraph` containing the ReAct loop:

```
handler ⇄ ToolNode → END (when no tool calls)
(per turn)
```

- `handler()` runs per turn, receives injected kwargs based on its signature
- `ToolNode` contains ALL tools (user tools + Skill + SkillRead)
- `llm` is pre-bound with all tools before each handler call
- Loop continues until handler returns a response with no tool calls
- From outside, the subgraph looks like a single node

### Skill frontmatter (simplified)

No `allowed-tools`. Skills are purely about instruction delivery and reference files.

```markdown
---
name: market-sizing
description: Calculate TAM, SAM, and SOM for market analysis
---
# Market Sizing Methodology
Step 1: Define market boundaries...
Step 2: Top-down analysis...

Use `SkillRead("market-sizing", "calculator.py")` to view the template.
```

### Tool binding

- All tools (user tools + Skill + SkillRead) are bound to the LLM from turn 1
- No progressive tool unlocking — all tools visible immediately
- `Skill` tool still provides progressive **instruction disclosure** (methodology, steps, context)
- ToolNode has the full tool pool at init

### Usage patterns

#### Standalone execution

```python
researcher.invoke(
    {"messages": [HumanMessage("Size the SaaS market")]},
    context=AppContext(user_id="alice", db=db),
)
```

#### As a node in a multi-agent graph

```python
workflow = StateGraph(AgentState, context_schema=AppContext)
workflow.add_node("researcher", researcher)
workflow.add_node("analyst", analyst)
workflow.add_conditional_edges(...)
graph = workflow.compile()
```

#### Minimal node (no tools, no skills)

```python
class summarizer(node):
    llm = ChatOpenAI(model="gpt-4o-mini")

    async def handler(state, *, llm):
        messages = [SystemMessage("Summarize.")] + state["messages"]
        response = await llm.ainvoke(messages)
        return {"messages": [response], "sender": "summarizer"}
```

No tools → no ToolNode → no loop → handler runs once.

#### Manual path with SkillKit (no metaclass)

```python
from langchain_skillkit import SkillKit, AgentState
from langgraph.graph import StateGraph, START, END
from langgraph.prebuilt import ToolNode

skill_kit = SkillKit("skills/")
all_tools = [web_search, calculate] + skill_kit.get_tools()

def researcher(state):
    bound_llm = llm.bind_tools(all_tools)
    response = bound_llm.invoke(state["messages"])
    return {"messages": [response], "sender": "researcher"}

workflow = StateGraph(AgentState)
workflow.add_node("researcher", researcher)
workflow.add_node("tools", ToolNode(all_tools))
workflow.add_conditional_edges("researcher", should_continue, ["tools", END])
workflow.add_edge("tools", "researcher")
graph = workflow.compile()
```

#### Shared SkillKit across nodes

```python
shared_skills = SkillKit(["skills/", "shared_skills/"])

class researcher(node):
    llm = ChatOpenAI(model="gpt-4o")
    tools = [web_search]
    skills = shared_skills  # accepts SkillKit instance or str or list[str]

    async def handler(state, *, llm): ...

class analyst(node):
    llm = ChatOpenAI(model="gpt-4o")
    tools = [sql_query]
    skills = shared_skills  # same SkillKit, shared

    async def handler(state, *, llm): ...
```

### Naming decisions

- Package: `langchain-skillkit`
- Toolkit class: `SkillKit` (extends LangChain `BaseToolkit`)
- Base class: `node` (lowercase, matches LangGraph terminology, mirrors `route` from fastapi-filebased-routing)
- Handler method: `handler` (mandatory, one turn of reasoning)
- Skill tool: `Skill` (loads a skill's instructions, lists available skills)
- Skill read tool: `SkillRead` (reads a reference file scoped to the skill's own directory)

### Separation of concerns

| Skill markdown | Python class |
|---|---|
| `name`, `description` | `llm` (which model) |
| Instructions (methodology, steps) | `tools` (available tool instances) |
| Reference files (templates, scripts) | `skills` (directory paths or SkillKit instance) |
| | `handler()` (how the LLM is called) |

**Markdown** = domain expertise (what to do, how to think)
**Python** = runtime wiring (which LLM, which tools, how to call)

### Inspiration

- **fastapi-filebased-routing**: metaclass factory pattern (`_RouteMeta` → `RouteConfig`), convention-over-configuration
- **LangGraph Runtime**: `Runtime[ContextT]` for dependency injection, `context_schema` for typed context
- **LangGraph create_react_agent**: subgraph pattern, ReAct loop internals
- **LangChain BaseToolkit**: `get_tools()` pattern for tool providers

## LangGraph Alignment

Reviewed against LangGraph source code (langgraph 0.4+).

### Aligned

- `CompiledStateGraph` as node in parent graph — natively supported
- `Runtime[ContextT]` flows through subgraphs automatically via config
- `BaseToolkit` extension for SkillKit — idiomatic LangChain
- Handler as per-turn function with framework loop — matches `create_react_agent` internals
- Handler signature `(state, *, keyword_args)` — matches LangGraph node convention
- All tools bound from turn 1, ToolNode has full pool — simplest correct approach

### Implementation notes

1. **ToolNode gets full tool pool.** User tools + Skill + SkillRead all in ToolNode at init. LLM bound with all of them from the start.

2. **Runtime is keyword-only.** The metaclass generates a LangGraph-compatible node `(state, *, runtime)` that wraps the handler, inspects its signature, and injects only requested kwargs.

3. **Lazy compilation.** Metaclass runs at import time but defers graph compilation. Stores class attributes; compiles `CompiledStateGraph` on first `.invoke()` / `.ainvoke()`.

4. **Signature-based injection.** Metaclass inspects handler parameter names via `inspect.signature()`. Unknown parameter names raise `ValueError` at compilation time. Only `state`, `llm`, `tools`, `runtime` are valid.

## Implementation plan

### New files

| File | Purpose |
|---|---|
| `src/langchain_skillkit/node.py` | `_NodeMeta` metaclass, `node` base class, lazy `CompiledStateGraph` builder |

### Renamed/modified files

| File | Changes |
|---|---|
| Package directory | `langchain_agent_skills/` → `langchain_skillkit/` |
| `skill_toolkit.py` | Rename class to `SkillKit`. Rename `read_reference` → `SkillRead`. Remove `unlocked_tools` set. Remove progressive tool unlock logic from `Skill` tool. |
| `validate.py` | Validate `node` class body (handler required). Validate handler signature params. |
| `types.py` | Remove `NodeConfig`. Keep `SkillConfig`. Remove `allowed-tools` parsing from `SkillConfig`. |
| `create_agent.py` | Remove (replaced by `node` metaclass). |
| `__init__.py` | Export `SkillKit`, `node`, `AgentState`. |
| `state.py` | Keep as-is. |
| `frontmatter.py` | Keep as-is (still needed for SKILL.md parsing). |

### Test changes

All existing tests rewritten. New test cases:

- `SkillKit` returns `[Skill, SkillRead]` from `get_tools()`
- `SkillKit` accepts `str | list[str]` for directories
- `SkillKit` works standalone (no metaclass)
- Metaclass produces `CompiledStateGraph`
- `handler()` required — error if missing
- Handler signature injection — only requested params injected
- Unknown handler params — error at compilation
- `SkillRead` tool name and scoping
- Standalone invocation (`researcher.invoke(...)`)
- Node-in-parent-graph composition
- Lazy compilation (no error at import, error at invocation if misconfigured)
- No tools → no ToolNode → handler runs once (no loop)
- Shared `SkillKit` across multiple nodes
- `Skill` returns instructions (no tool unlock)

## Parking lot (future considerations)

- Post-model hooks (transform LLM output before returning to state)
- Template engine for prompts (Mustache, Jinja2, or simple str.format)
- Multi-agent convenience functions (orchestrator patterns)
- Backward compatibility shim for `create_agent()` users
- `config: RunnableConfig` as injectable handler param
- `store: BaseStore` as injectable handler param (shortcut for `runtime.store`)
